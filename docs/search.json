[
  {
    "objectID": "quarto/projects.html",
    "href": "quarto/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "quarto/projects.html#welcome-to-my-stat-515-mid-project",
    "href": "quarto/projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "quarto/projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "quarto/projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  },
  {
    "objectID": "Midproject/projects.html",
    "href": "Midproject/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -"
  },
  {
    "objectID": "Midproject/projects.html#welcome-to-my-stat-515-mid-project",
    "href": "Midproject/projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -"
  },
  {
    "objectID": "Midproject/projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "Midproject/projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  },
  {
    "objectID": "java/index.html",
    "href": "java/index.html",
    "title": "Deepshika Saravanan",
    "section": "",
    "text": "MSDA Graduate student @ George Mason University | Software Engineer | Former Senior Analyst @ Capgemini Technology Services | Python | SQL | Bigdata Analytics | Machine Learning | Data Visualisation\nWith a solid foundation in Data Science, I have honed my abilities to extract, clean, analyze, and visualize data to derive actionable insights. My experience includes working with diverse datasets, from structured databases to unstructured text, and applying advanced analytics techniques to uncover hidden patterns and trends.\nMy goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence."
  },
  {
    "objectID": "java/about.html",
    "href": "java/about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, Everyone!\nMy official registration name is Deepshika Saravanan. I hold a Bachelor’s degree in Technology in department Computer Science from Chennai, India from SRM Institute of Science and Technology, 2 years ago. My educational goals for this course include developing a deep connection with big data analytics, mastering data analysis techniques, and gaining proficiency in deploying relevant tools. In the broader context of my Mason program of study, I aim to build a strong foundation in data management and mining, as well as enhance my skills in data visualization for analytics. In addition to the current course, I am taking Principles of Data Management/Mining and Visualization for Analytics this semester.\nI have experience working in various computing environments, including both local and cloud-based platforms. Locally, I am proficient in operating systems such as Windows and Linux. In terms of cloud services, I don’t have hands-on experience with AWS (Amazon Web Services) but have done few courses on online for an overview experience of the concepts. My familiarity extends to utilizing cloud resources for storage, computation, and deploying applications in minor projects. While I may not consider myself an expert, I have a solid understanding of cloud computing concepts.\nAt this moment, I don’t have specific concerns about taking this course. However, I am open to addressing any challenges that may arise during the course and am committed to actively engaging with the material to ensure a successful learning experience throughout the course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepshika Saravanan",
    "section": "",
    "text": "MSDA Graduate student @ George Mason University | Software Engineer | Former Senior Analyst @ Capgemini Technology Services | Python | SQL | Bigdata Analytics | Machine Learning | Data Visualisation\nWith a solid foundation in Data Science, I have honed my abilities to extract, clean, analyze, and visualize data to derive actionable insights. My experience includes working with diverse datasets, from structured databases to unstructured text, and applying advanced analytics techniques to uncover hidden patterns and trends."
  },
  {
    "objectID": "index.html#my-goal-is-to-leverage-my-passion-for-data-and-my-skills-in-data-science-to-make-a-meaningful-impact.-whether-its-optimizing-business-operations-improving-customer-experiences-or-driving-strategic-decision-making-i-am-dedicated-to-using-data-to-drive-positive-change-and-drive-towards-excellence.",
    "href": "index.html#my-goal-is-to-leverage-my-passion-for-data-and-my-skills-in-data-science-to-make-a-meaningful-impact.-whether-its-optimizing-business-operations-improving-customer-experiences-or-driving-strategic-decision-making-i-am-dedicated-to-using-data-to-drive-positive-change-and-drive-towards-excellence.",
    "title": "Deepshika Saravanan",
    "section": "My goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence.",
    "text": "My goal is to leverage my passion for data and my skills in Data Science to make a meaningful impact. Whether its optimizing business operations, improving customer experiences, or driving strategic decision-making, I am dedicated to using data to drive positive change and drive towards excellence."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, Everyone!\nMy official registration name is Deepshika Saravanan. I hold a Bachelor’s degree in Technology in department Computer Science from Chennai, India from SRM Institute of Science and Technology, 2 years ago. My educational goals for this course include developing a deep connection with big data analytics, mastering data analysis techniques, and gaining proficiency in deploying relevant tools. In the broader context of my Mason program of study, I aim to build a strong foundation in data management and mining, as well as enhance my skills in data visualization for analytics. In addition to the current course, I am taking Principles of Data Management/Mining and Visualization for Analytics this semester.\nI have experience working in various computing environments, including both local and cloud-based platforms. Locally, I am proficient in operating systems such as Windows and Linux. In terms of cloud services, I don’t have hands-on experience with AWS (Amazon Web Services) but have done few courses on online for an overview experience of the concepts. My familiarity extends to utilizing cloud resources for storage, computation, and deploying applications in minor projects. While I may not consider myself an expert, I have a solid understanding of cloud computing concepts.\nAt this moment, I don’t have specific concerns about taking this course. However, I am open to addressing any challenges that may arise during the course and am committed to actively engaging with the material to ensure a successful learning experience throughout the course."
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "The codes that we wrote in R programming language to represent visualizations for STAT-515 MID-PROJECT.\nHere’s an PDF of Visualisations :Download PDFHere’s an embedded PDF of Code .:Download PDF"
  },
  {
    "objectID": "finalproj.html",
    "href": "finalproj.html",
    "title": "Final Project",
    "section": "",
    "text": "In the landscape of renewable energy and environmental conservation, precise and accurate data are paramount. The New York State Energy Research and Development Authority (NYSERDA) presents the 2023 Soils Data, a vital resource tailored for stakeholders in the Large-Scale Renewables and NY-Sun Programs. Released on January 12, 2024, this dataset supports the Renewable Energy Standard (RES) Request for Proposal (RFP) processes, providing essential data to inform site selection and development while prioritizing the conservation of high-quality agricultural soils in New York State.\nDataset Overview:\nThe NYSERDA 2023 Soils Data integrates multiple sources to provide a comprehensive tabular representation of soil types across New York. The dataset transforms the New York State Agricultural Land Classification’s master list into an accessible format, aligning each soil type with a unique identifier. This identifier links directly to the Natural Resources Conservation Service (NRCS) SSURGO database, facilitating geographic mapping and analysis in GIS software. It’s crucial to utilize the latest SSURGO database update for accurate spatial joining.\nDetailed Column Descriptions\n\nCounty: This column lists the name of the county in which the soil unit is located. It helps users identify and segregate data geographically within New York State.\nCounty_MAPSYM: A combination of the county name and the MAPSYM abbreviation, providing a unique identifier for each soil unit. This facilitates easier cross-referencing within the dataset and ensures distinct entries for each soil type.\nMAPSYM: Contains the abbreviation for the soil unit name. This abbreviated form helps in quick referencing and mapping in GIS systems.\nMUKEY: Stands for “Map Unit Key,” a unique identifier for each soil unit used to join this dataset to the NRCS SSURGO database. Blank cells indicate map symbols no longer present in the SSURGO database, which highlights updates or changes in soil classification.\nDefault Mineral Soil Group: Indicates the Mineral Soil Group (MSG) classification for each soil unit, specifically for NYSERDA’s program use. MSG values range from 1 to 10, with lower numbers indicating higher quality agricultural soils. This classification aids in assessing the agricultural value and potential environmental impact of developing a particular area.\nMultiple MSG Flag: Indicates if there are multiple MSG values for a soil unit. This is critical for understanding the variability within a single mapped unit and planning for site-specific mitigation strategies.\nFlag - MSG Values: Lists all possible MSG values for a given soil unit where multiple classifications exist, providing a comprehensive view of the soil’s characteristics.\nFlag - Fields: Identifies fields within the dataset that contain differing data across subsets of soil types, which is important for data integrity checks and understanding dataset complexity.\nCapability Class (FM5 CAP): Represents the land capability classification, which categorizes land based on its highest sustainable use and risk of damage if used for agriculture. This helps in evaluating the suitability of land for agricultural versus developmental purposes.\nSoil Temp. Regime: Describes the soil temperature regime as defined in the Soil Taxonomy, which influences what types of vegetation the soil can support naturally.\nSoil Modifier: Describes any element of the soil unit that might be used for altering the soil group value, providing insights into how soil characteristics might be adjusted or reported differently in agricultural assessments.\nSoil Slope: Indicates the range of slope percentage for the soil unit, which is crucial for determining erosion risk and suitability for various types of land use.\nSoil Name: Provides the standard name of the soil unit, which can be cross-referenced with other agricultural or ecological databases.\nDrainage: Identifies the natural drainage condition of the soil, crucial for understanding water management needs and potential flooding risks.\nModifier: Similar to “Soil Modifier”, it describes elements that could alter the classification or characteristics of the soil unit.\nTexture: Describes the general texture of the soil’s parent material, impacting its permeability, fertility, and suitability for different types of crops.\nFlooding: Notes whether there’s a risk of flooding from nearby water bodies, which can affect crop selection and land use planning.\nLime: Indicates if lime application is recommended for the soil unit to correct pH levels, which is vital for optimizing crop health and yield.\nRotation: Specifies the number of years certain crops (like corn) can be grown in rotation without exceeding permissible soil loss rates, guiding sustainable agricultural practices.\nCorn Yield (ton/acre): Lists average management yields of corn per acre, useful for economic planning and agricultural forecasting.\nHay Yield (ton/acre): Provides the average management yields of hay per acre, aiding in the assessment of the soil’s productivity for hay cultivation.\nChange: Documents any changes in the information for this soil map unit, which could influence soil classification or usage recommendations.\nTDN (ton/acre): Stands for Total Digestible Nutrients, calculating nutritional value based on crop yields, crucial for assessing soil fertility and potential agricultural productivity.\nIndex (TDN): Represents an index value of Total Digestible Nutrients, providing a comparative measure of the soil’s productivity potential against the best soil in the dataset.\n\n\n\nLoading the dataset and structure and summary of data\n\n\nShow the code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\nlibrary(readr)\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n\nShow the code\n# Load the dataset\ndata &lt;- read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n\nRows: 8513 Columns: 24\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n# Check the structure and summary of data\nstr(data)\n\n\nspc_tbl_ [8,513 × 24] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ County                    : chr [1:8513] \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n $ County_MAPSYM             : chr [1:8513] \"NY001_AE\" \"NY001_ANA\" \"NY001_ANB\" \"NY001_ANC\" ...\n $ MAPSYM                    : chr [1:8513] \"Ae\" \"AnA\" \"AnB\" \"AnC\" ...\n $ MUKEY                     : num [1:8513] 288688 288689 288690 288691 288692 ...\n $ Default Mineral Soil Group: num [1:8513] 7 6 6 7 8 8 9 7 5 5 ...\n $ Multiple MSG Flag         : chr [1:8513] NA NA NA NA ...\n $ Flag - MSG Values         : num [1:8513] NA NA NA NA NA NA NA NA NA NA ...\n $ Flag - Fields             : chr [1:8513] NA NA NA NA ...\n $ Capability Class (FM5 CAP): chr [1:8513] \"4W\" \"3W\" \"3W\" \"3E\" ...\n $ Soil Temp. Regime         : chr [1:8513] \"Mesic\" \"Mesic\" \"Mesic\" \"Mesic\" ...\n $ Soil Modifier             : chr [1:8513] NA NA NA NA ...\n $ Soil Slope                : chr [1:8513] \"\\\"00-03\\\"\" \"\\\"00-03\\\"\" \"\\\"03-08\\\"\" \"\\\"08-15\\\"\" ...\n $ Soil Name                 : chr [1:8513] \"ALLIS\" \"ANGOLA\" \"ANGOLA\" \"ANGOLA\" ...\n $ Drainage                  : chr [1:8513] \"Poorly Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" ...\n $ Modifier                  : chr [1:8513] NA NA NA NA ...\n $ Texture                   : chr [1:8513] \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" ...\n $ Flooding                  : chr [1:8513] NA NA NA NA ...\n $ Lime                      : chr [1:8513] \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" ...\n $ Rotation                  : num [1:8513] 2 3 3 1 2 3 0 2 4 4 ...\n $ Corn Yield (ton/acre)     : num [1:8513] 7.5 12 12 11.2 6 ...\n $ Hay Yield (ton/acre)      : num [1:8513] 1.6 1.95 1.95 1.6 0.96 0 0 1.6 2.64 2.64 ...\n $ Change                    : chr [1:8513] NA NA NA NA ...\n $ TDN (ton/acre)            : num [1:8513] 0.94 1.4 1.4 0.95 0.48 0 0 1 1.81 1.81 ...\n $ Index (TDN)               : num [1:8513] 20.7 30.9 30.9 20.8 10.6 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   County = col_character(),\n  ..   County_MAPSYM = col_character(),\n  ..   MAPSYM = col_character(),\n  ..   MUKEY = col_double(),\n  ..   `Default Mineral Soil Group` = col_double(),\n  ..   `Multiple MSG Flag` = col_character(),\n  ..   `Flag - MSG Values` = col_double(),\n  ..   `Flag - Fields` = col_character(),\n  ..   `Capability Class (FM5 CAP)` = col_character(),\n  ..   `Soil Temp. Regime` = col_character(),\n  ..   `Soil Modifier` = col_character(),\n  ..   `Soil Slope` = col_character(),\n  ..   `Soil Name` = col_character(),\n  ..   Drainage = col_character(),\n  ..   Modifier = col_character(),\n  ..   Texture = col_character(),\n  ..   Flooding = col_character(),\n  ..   Lime = col_character(),\n  ..   Rotation = col_double(),\n  ..   `Corn Yield (ton/acre)` = col_double(),\n  ..   `Hay Yield (ton/acre)` = col_double(),\n  ..   Change = col_character(),\n  ..   `TDN (ton/acre)` = col_double(),\n  ..   `Index (TDN)` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "finalproj.html#exploratory-data-analysis-of-a-dataset",
    "href": "finalproj.html#exploratory-data-analysis-of-a-dataset",
    "title": "Final Project",
    "section": "",
    "text": "Loading the dataset and structure and summary of data\n\n\nShow the code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\nlibrary(readr)\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n\nShow the code\n# Load the dataset\ndata &lt;- read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n\nRows: 8513 Columns: 24\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n# Check the structure and summary of data\nstr(data)\n\n\nspc_tbl_ [8,513 × 24] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ County                    : chr [1:8513] \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n $ County_MAPSYM             : chr [1:8513] \"NY001_AE\" \"NY001_ANA\" \"NY001_ANB\" \"NY001_ANC\" ...\n $ MAPSYM                    : chr [1:8513] \"Ae\" \"AnA\" \"AnB\" \"AnC\" ...\n $ MUKEY                     : num [1:8513] 288688 288689 288690 288691 288692 ...\n $ Default Mineral Soil Group: num [1:8513] 7 6 6 7 8 8 9 7 5 5 ...\n $ Multiple MSG Flag         : chr [1:8513] NA NA NA NA ...\n $ Flag - MSG Values         : num [1:8513] NA NA NA NA NA NA NA NA NA NA ...\n $ Flag - Fields             : chr [1:8513] NA NA NA NA ...\n $ Capability Class (FM5 CAP): chr [1:8513] \"4W\" \"3W\" \"3W\" \"3E\" ...\n $ Soil Temp. Regime         : chr [1:8513] \"Mesic\" \"Mesic\" \"Mesic\" \"Mesic\" ...\n $ Soil Modifier             : chr [1:8513] NA NA NA NA ...\n $ Soil Slope                : chr [1:8513] \"\\\"00-03\\\"\" \"\\\"00-03\\\"\" \"\\\"03-08\\\"\" \"\\\"08-15\\\"\" ...\n $ Soil Name                 : chr [1:8513] \"ALLIS\" \"ANGOLA\" \"ANGOLA\" \"ANGOLA\" ...\n $ Drainage                  : chr [1:8513] \"Poorly Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" \"Somewhat Well-Drained\" ...\n $ Modifier                  : chr [1:8513] NA NA NA NA ...\n $ Texture                   : chr [1:8513] \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" \"Silty Loam\" ...\n $ Flooding                  : chr [1:8513] NA NA NA NA ...\n $ Lime                      : chr [1:8513] \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" \"Requires lime additions within every rotation\" ...\n $ Rotation                  : num [1:8513] 2 3 3 1 2 3 0 2 4 4 ...\n $ Corn Yield (ton/acre)     : num [1:8513] 7.5 12 12 11.2 6 ...\n $ Hay Yield (ton/acre)      : num [1:8513] 1.6 1.95 1.95 1.6 0.96 0 0 1.6 2.64 2.64 ...\n $ Change                    : chr [1:8513] NA NA NA NA ...\n $ TDN (ton/acre)            : num [1:8513] 0.94 1.4 1.4 0.95 0.48 0 0 1 1.81 1.81 ...\n $ Index (TDN)               : num [1:8513] 20.7 30.9 30.9 20.8 10.6 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   County = col_character(),\n  ..   County_MAPSYM = col_character(),\n  ..   MAPSYM = col_character(),\n  ..   MUKEY = col_double(),\n  ..   `Default Mineral Soil Group` = col_double(),\n  ..   `Multiple MSG Flag` = col_character(),\n  ..   `Flag - MSG Values` = col_double(),\n  ..   `Flag - Fields` = col_character(),\n  ..   `Capability Class (FM5 CAP)` = col_character(),\n  ..   `Soil Temp. Regime` = col_character(),\n  ..   `Soil Modifier` = col_character(),\n  ..   `Soil Slope` = col_character(),\n  ..   `Soil Name` = col_character(),\n  ..   Drainage = col_character(),\n  ..   Modifier = col_character(),\n  ..   Texture = col_character(),\n  ..   Flooding = col_character(),\n  ..   Lime = col_character(),\n  ..   Rotation = col_double(),\n  ..   `Corn Yield (ton/acre)` = col_double(),\n  ..   `Hay Yield (ton/acre)` = col_double(),\n  ..   Change = col_character(),\n  ..   `TDN (ton/acre)` = col_double(),\n  ..   `Index (TDN)` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "finalproj.html#r-code-for-multiple-regression-with-interaction-terms.",
    "href": "finalproj.html#r-code-for-multiple-regression-with-interaction-terms.",
    "title": "Final Project",
    "section": "R Code for Multiple Regression with Interaction Terms.",
    "text": "R Code for Multiple Regression with Interaction Terms.\nWe’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\nInterpretation of the Regression Output Coefficients:\nIntercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding).\nDrainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant.\nTexture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant.\nDrainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding.\nStatistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect.\nModel Fit:\nResidual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units.\nMultiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data.\nAdjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit.\nResiduals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\nRandom Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\nInterpretation of the Random Forest Model Results:\nModel Performance:\nMean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy.\n% Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%).\nFeature Importance:\nDrainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding.\nTexture: Importance score of 121.6274, slightly more influential than ‘Drainage’.\nDrainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features.\nComparison and Conclusion:\nExplanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors.\nPredictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual.\nFeature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects.\n\nCross-Validation for Linear Regression and Random Forest\n\n\nThe cross-validation results for both the Linear Regression and Random Forest models provide a clear comparison of their performance:\nLinear Regression Model:\nRMSE (Root Mean Squared Error): 0.6608545\nR-squared: 0.5871187\nMAE (Mean Absolute Error): 0.4143339\nThese metrics indicate that the linear regression model explains about 58.71% of the variance in the data. The RMSE and MAE values provide a measure of the average error in the predictions.\nRandom Forest Model:\nRMSE: 0.4282331\nR-squared: 0.8134469\nMAE: 0.1653817\nOptimal mtry: 2\nThe Random Forest model significantly outperforms the Linear Regression in all the metrics. It explains approximately 81.34% of the variance in the data, and both its RMSE and MAE are lower, indicating more accurate predictions.\n\nThe diagnostic plots for your linear regression model provide valuable insights into how well the model meets the assumptions necessary for optimal performance. Here’s an interpretation of each plot:\n\n\nResiduals vs Fitted: This plot helps check for non-linearity and heteroscedasticity (unequal variance of residuals).\n\nObservations: The residuals do not appear to display any clear pattern, which is good for linearity. However, there is a slight “fanning” effect where the spread of residuals increases with fitted values, suggesting potential heteroscedasticity.\nAction: Consider transformations of the dependent variable or use heteroscedasticity-consistent standard errors if this model will be used for inferential purposes.\n2. Normal Q-Q This plot shows if the residuals are normally distributed—a key assumption of linear regression.\nObservations: Most points lie on the line, but there are deviations at the tails (both lower and upper ends), indicating slight departures from normality.\nAction: This is generally not severe unless very precise estimates are required. For more robustness, consider using non-parametric bootstrapping techniques to estimate standard errors.\n3. Scale-Location (or Spread-Location) This plot checks if residuals are spread equally along the ranges of predictors (homoscedasticity).\nObservations: The red line (a loss fit) shows a trend, which suggests that residuals have non-constant variance across the range of fitted values.\nAction: This supports the earlier suggestion of possible heteroscedasticity. Transformations or robust regression methods might be needed.\n4. Residuals vs Leverage This plot helps to identify influential cases that might have an unduly large effect on the model estimate.\nObservations: Most data points have low leverage, but there are a few points well outside the Cook’s distance lines (notably the points labeled 7280 and 4430).\nAction: Investigate these points further to determine if they are outliers or influential points due to data entry errors or other reasons. Consider removing or adjusting these points if they are deemed to be errors. Conclusion The diagnostic plots indicate that while the model does not suffer from severe issues, there are indications of potential heteroscedasticity and some influence from outliers or high-leverage points."
  },
  {
    "objectID": "finalproj.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "href": "finalproj.html#research-question-3-how-does-the-presence-of-multiple-mineral-soil-group-multiple-msg-flag-affect-corn-and-hay-yield-across-different-counties",
    "title": "Final Project",
    "section": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?",
    "text": "Research Question 3: How does the presence of multiple mineral soil group (Multiple MSG Flag) affect corn and hay yield across different counties?\nThe aim of this question is to analyze the influence of the Multiple MSG Flag on the yield of corn and hay, and to determine if there are significant differences in yield based on this soil classification.\nVariables:\nDependent Variables: Corn yield and hay yield (measured in tons per acre).\nIndependent Variable: Multiple MSG Flag (indicating presence of multiple mineral soil groups absence).\n\n\nShow the code\n# Load necessary libraries\nlibrary(readr)  # For reading CSV files\nlibrary(dplyr)  # For data manipulation\nlibrary(ggplot2)  # For creating visualizations\n\n# dataset\ndata = read_csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n\nRows: 8513 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): County, County_MAPSYM, MAPSYM, Multiple MSG Flag, Flag - Fields, C...\ndbl  (8): MUKEY, Default Mineral Soil Group, Flag - MSG Values, Rotation, Co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n# Check all column names in the dataset\ncolnames(data)\n\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default Mineral Soil Group\" \"Multiple MSG Flag\"         \n [7] \"Flag - MSG Values\"          \"Flag - Fields\"             \n [9] \"Capability Class (FM5 CAP)\" \"Soil Temp. Regime\"         \n[11] \"Soil Modifier\"              \"Soil Slope\"                \n[13] \"Soil Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn Yield (ton/acre)\"     \n[21] \"Hay Yield (ton/acre)\"       \"Change\"                    \n[23] \"TDN (ton/acre)\"             \"Index (TDN)\"               \n\n\n\n\nShow the code\nlibrary(plotly)\n\n# Create ggplot for Corn Yield\np_corn &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Corn Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"blue\") +\n  labs(title = \"Corn Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Corn Yield\")\n\n# Convert to plotly\nplotly_corn &lt;- ggplotly(p_corn)\n\n# Display the plots\nplotly_corn\n\n\n\n\n\n\nBox Plot Analysis for “Yes” Flag in Corn Yield Minimum Yield:\nThe minimum yield recorded is 0, indicating that there were instances where no corn was produced.\nFirst Quartile (Q1): The first quartile is at 9.00 tons/acre, which means that 25% of the yields are below this value. This lower quartile suggests a significant number of plots produced yields less than 9 tons per acre.\nMedian (Q2): The median yield is 10.50 tons/acre, indicating that half of the yields are below this value and half are above. This median is fairly low compared to the maximum, suggesting a skewed distribution with a clustering of yields around the lower mid-range.\nThird Quartile (Q3): The third quartile is at 18.48 tons/acre, showing that 75% of the yields are below this level. This wider spread between the median and Q3 indicates variability in the higher yield range.\nMaximum Yield: The maximum yield observed is 24.00 tons/acre, significantly lower than the maximum yields observed in the “No” condition, suggesting that while the “Yes” flag might include high-yield scenarios, its peak potential is less than that of the “No” flag scenarios.\nMinimum Yield: The minimum yield starts at 0, similar to the “Yes” category, indicating that there were also no-yield instances under the “No” condition.\nFirst Quartile (Q1): Again, it is close to zero which suggests a significant portion of the data also produced very low yields.\nMedian Yield: The median is noticeably lower than in the “Yes” condition, at about 12.00 tons/acre. Third Quartile (Q3): Positioned around 13.68 tons/acre, which is lower than the third quartile in the “Yes” category, indicating less variability in the higher yield segment under the “No” condition. Maximum Yield: The maximum yield extends up to about 40.56 tons/acre, which is similar to the “Yes” condition, indicating the highest yields are comparable between the two groups. However, these very high yields are outliers, as shown by the points above the upper fence (31.50 tons/acre).\n\n\nShow the code\n# Create ggplot for Hay Yield\np_hay &lt;- ggplot(data, aes(x = `Multiple MSG Flag`, y = `Hay Yield (ton/acre)`)) +\n  geom_boxplot(fill = \"green\") +\n  labs(title = \"Hay Yield by Multiple MSG Flag\", x = \"Multiple MSG Flag\", y = \"Hay Yield\")\n\nplotly_hay &lt;- ggplotly(p_hay)\nplotly_hay\n\n\n\n\n\n\nBox Plot Analysis for Hay Yield For “Yes”: Minimum Yield: The minimum yield starts at 0, indicating that some plots did not produce any hay.\nFirst Quartile (Q1): The first quartile is around 0 as well, suggesting that at least 25% of the plots produced very little to no hay. Median Yield: The median yield is 1.63 tons/acre, indicating that half of the plots under the “Yes” condition produced this amount or less. This is a relatively low median, suggesting moderate production overall but lower than might be expected.\nThird Quartile (Q3): The third quartile at 3.35 tons/acre shows that 75% of the plots produced 3.35 tons/acre or less. This wider spread from the median to Q3 indicates variability in the higher production range.\nMaximum Yield: The maximum yield reported is 4.62 tons/acre, which is quite low compared to corn yield outliers. This suggests that even the highest-producing hay plots under the “Yes” flag aren’t exceedingly productive.\nBox Plot Analysis for “No” Flag in Hay Yield Minimum Yield: The minimum yield starts at 0, similar to the “Yes” condition, indicating that some plots under this condition also did not produce any hay.\nFirst Quartile (Q1): Positioned at 0.96 tons/acre, this marks where 25% of the yields fall below. This value suggests a base level of production that is slightly higher than the absolute minimum, indicating a better performance in the lower yield quartile compared to the “Yes” condition. Median Yield: The median yield is 1.95 tons/acre, which is exactly the same as in the “Yes” condition. This indicates a similar central tendency in terms of yield across both soil group conditions.\nThird Quartile (Q3): The third quartile is 2.68 tons/acre, showing that 75% of the data fall below this yield. The range between the first and third quartile is narrower compared to the “Yes” condition, indicating less variability in the higher yield ranges. Upper Fence: The upper fence at 4.02 tons/acre sets the boundary for what is considered normal yield variation; yields beyond this are treated as outliers. Maximum Yield: The maximum observed yield is 8.04 tons/acre, which is a significant outlier and suggests that a few plots can produce substantially more than typical plots under the “No” condition.\nLinear Regression Models :\n\nModel for Corn Yield:\nCoefficients: The intercept was significant at 12.5918 tons/acre (p &lt; 2e-16), indicating the average yield when the flag is “Yes”. The coefficient for “No” was -2.8169 (p &lt; 2e-16), indicating a significant reduction in yield when the flag is “No”. Model Fit: The model explained only 1.019% of the variance in yield (R-squared = 0.01019), suggesting that other factors besides the MSG flag are more influential in determining corn yield.\n\n\nModel for Hay Yield:\nCoefficients: The intercept for “Yes” was 2.31247 tons/acre (p &lt; 2e-16), with the “No” flag associated with a decrease in yield by 0.43985 tons/acre (p &lt; 9.6e-14).\nModel Fit: The R-squared value was 0.006494, indicating that the model explains a very small portion of the variance in hay yields.\nThe presence of multiple mineral soil groups (Multiple MSG Flag) positively affects both corn and hay yields. Our analysis shows that fields with Multiple mineral soil groups tend to yield higher compared to fields without such diversity. Specifically, regression analysis revealed significant increases in yield for areas with multiple soil groups: corn yields decrease by about 2.817 tons per acre and hay yields by about 0.440 tons per acre in the absence of soil diversity.\nBoth models show that the “Multiple MSG Flag” being “No” is associated with a decrease in yield for both corn and hay compared to when the flag is “Yes”. However, the effect is stronger and more pronounced for corn. Both models explain a very small fraction of the variability in yields, which suggests that there are many other factors affecting yield that are not captured by this variable alone."
  },
  {
    "objectID": "finalproj.html#references",
    "href": "finalproj.html#references",
    "title": "Final Project",
    "section": "References:",
    "text": "References:\nDataset :\nFor reference click this link to redirected to the dataset:\nhttps://data.ny.gov/Energy-Environment/NYSERDA-2023-Soils-Data-for-use-in-the-Large-Scale/dayw-t2bj/about_data\nOverview of the dataset: https://data.ny.gov/api/views/dayw-t2bj/files/eda3c994-48d7-4f59-8ee4-5e8c7a022b83?download=true&filename=NYSERDA_Soils2023_Overview.pdf\nDescription for the columns: https://data.ny.gov/api/views/dayw-t2bj/files/a9db1226-91d1-40f6-ae9c-ee4a34b20b67?download=true&filename=NYSERDA_Soils2023_DataDictionary.pdf\nhttps://www.mdpi.com/2073-4395/12/11/2613\nhttps://journalijpss.com/index.php/IJPSS/article/view/3685"
  },
  {
    "objectID": "influence.html",
    "href": "influence.html",
    "title": "test",
    "section": "",
    "text": "# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\nThe Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\n# Load necessary libraries\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(dplyr)\ncolnames(data)\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant. Drainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\n#R Code for Random Forest with Feature Engineering Random Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects."
  },
  {
    "objectID": "influence.html#r-markdown",
    "href": "influence.html#r-markdown",
    "title": "test",
    "section": "",
    "text": "# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))\ndata$Flooding &lt;- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata &lt;- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test &lt;- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n\n\nThe Pearson correlation coefficient between the ‘Drainage’ and ‘Flooding’ columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\nInterpretation and Next Steps: Statistical Significance: The p-value is less than 0.05 (actually, it’s much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low. Practical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It’s important to consider these factors in any risk assessment or land use planning. Further Analysis: Investigating how other variables interact with ‘Drainage’ and ‘Flooding’ could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\n# Load necessary libraries\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(dplyr)\ncolnames(data)\n\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean &lt;- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture &lt;- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class &lt;- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model &lt;- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n\n# Plot importance of variables\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\nInterpretation of the Variable Importance Plots: %IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, ‘Drainage’ seems to be the most important predictor, followed by ‘Capability_Class’ and ‘Texture’. IncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, ‘Capability_Class’ contributes most to node purity, followed by ‘Drainage’ and ‘Texture’. This suggests that ‘Capability_Class’ is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n#Investigating interaction effects between predictors like ‘Drainage’ and ‘Texture’ in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we’ll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\nR Code for Multiple Regression with Interaction Terms We’ll modify the linear regression model to include an interaction term between ‘Drainage’ and ‘Texture’. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\n# Load necessary library\nlibrary(stats)\n\ndata$Texture &lt;- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage &lt;- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction &lt;- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  &lt; 2e-16 ***\nTexture           0.294757   0.020929  14.084  &lt; 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,    Adjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of the Regression Output Coefficients: Intercept (-0.349160): This represents the baseline value of flooding when both ‘Drainage’ and ‘Texture’ are at their reference levels (typically zero in numerical coding). Drainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant. Texture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant. Drainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of ‘Drainage’ and ‘Texture’ on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding. Statistical Significance: All predictors, including the interaction term, are highly statistically significant (p &lt; 0.001), indicating strong evidence against the null hypothesis of no effect. Model Fit: Residual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units. Multiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data. Adjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit. Residuals: The spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\n#R Code for Random Forest with Feature Engineering Random Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata &lt;- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction &lt;- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n\n\nInterpretation of the Random Forest Model Results: Model Performance: Mean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy. % Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%). Feature Importance: Drainage: Importance score of 115.0117, suggesting it’s a significant predictor of flooding. Texture: Importance score of 121.6274, slightly more influential than ‘Drainage’. Drainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features. Comparison and Conclusion: Explanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors. Predictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual. Feature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "projects.html#welcome-to-my-stat-515-mid-project",
    "href": "projects.html#welcome-to-my-stat-515-mid-project",
    "title": "Projects",
    "section": "",
    "text": "Here is the Youtube link: -\nhttps://youtu.be/oRzrxBoEOOs"
  },
  {
    "objectID": "projects.html#title-using-r-to-redesign-statistical-graphs",
    "href": "projects.html#title-using-r-to-redesign-statistical-graphs",
    "title": "Projects",
    "section": "Title: Using R to Redesign Statistical Graphs:",
    "text": "Title: Using R to Redesign Statistical Graphs:\nData visualization is the graphical representation of data to help people understand the significance of data by summarizing and presenting it in a visual form. It enables decision-makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Through the use of charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\nData manipulation refers to the process of changing data to make it easier to read or to prepare it for further analysis. It involves various operations such as sorting, filtering, aggregating, and transforming data to meet specific requirements. Data manipulation is often done using programming languages like Python, R, or SQL, and tools like Excel or pandas. The goal of data manipulation is to ensure that data is in a format that is suitable for analysis and can provide meaningful insights.\nIn R programming, redesigning visualizations is important for several reasons, and there are specific techniques and packages that can be used to achieve this:\nClarity and Effectiveness: R offers a wide range of packages for creating visualizations, such as ggplot2 and plotly, which allow for the creation of clear and effective visualizations through customizable features like color schemes, labels, and annotations.\nAudience Understanding: By using the ggplot2 package in R, visualizations can be customized to suit different audiences. For example, the facet_wrap function can be used to create multiple plots based on different variables, allowing for easy comparison and understanding.\nData Changes: R provides tools for easily updating visualizations with new data. For instance, the update_geom function in ggplot2 can be used to change the data plotted on a graph without having to recreate the entire plot.\nImproved Aesthetics: R offers various themes and styling options through packages like ggthemes, which allow for the creation of visually appealing visualizations. Additionally, the ggplot2 package provides features for customizing plot elements such as fonts, colors, and grid lines.\nNew Insights: Redesigning visualizations in R can lead to the discovery of new insights in the data. For example, by experimenting with different plot types or adding new variables to a plot, new patterns or relationships in the data may become apparent.\nOverall, in R programming, redesigning visualizations is essential for ensuring that they effectively communicate data, are tailored to the audience, and provide meaningful insights. R’s flexibility and extensive visualization capabilities make it a powerful tool for creating and redesigning visualizations to meet these goals.\nThe STAT 515 course provided us with valuable insights into the power of data visualization tools. It highlighted how visualization can effectively communicate complex data and uncover meaningful insights. This course emphasized the importance of using tools like R to create impactful visualizations that can convey information clearly and engage audiences effectively.\n#References: -\n#Dataset of The Top 100 most valuable brands in 2020:-\nhttps://howmuch.net/sources/top-100-most-valuable-brands-2020\n#Dataset of Visualizing the maximum Gender pay gap across USA:-\nhttps://howmuch.net/articles/men-vs-women-comparing-income-by-industry"
  }
]