{
  "hash": "78ac50218093f1b10d6b2700f643f954",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"test\"\nauthor: \"Deepshika Saravanan\"\ndate: \"2024-05-06\"\noutput: html_document\n---\n\n\n\n\n## R Markdown\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary library\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\n# Read the dataset\ndata <- read.csv(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\NYSERDA_2023_Soils_Data_for_use_in_the_Large-Scale_Renewables_and_NY-Sun_Programs.csv\")\n\n# Convert factors to numeric if necessary (assuming 'Flooding' is binary or categorical)\ndata$Drainage <- as.numeric(as.factor(data$Drainage))\ndata$Flooding <- as.numeric(as.factor(data$Flooding))\n\n# Handling NA values\ndata <- na.omit(data)\n\n# Using cor.test to determine the correlation\ncor_test <- cor.test(data$Drainage, data$Flooding)\nprint(cor_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data$Drainage and data$Flooding\nt = 16.142, df = 407, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5618257 0.6804973\nsample estimates:\n      cor \n0.6247561 \n```\n\n\n:::\n:::\n\nThe Pearson correlation coefficient between the 'Drainage' and 'Flooding' columns in your dataset is approximately 0.625, indicating a moderate positive correlation. This suggests that as drainage characteristics of the soil increase (presumably indicating better drainage capacity), the occurrence of flooding also increases. This result might seem counterintuitive at first glance; however, it could indicate that areas classified with better drainage are also those where water accumulates quickly, thus making them prone to flooding under certain conditions.\n\nInterpretation and Next Steps:\nStatistical Significance: The p-value is less than 0.05 (actually, it's much smaller than that), indicating that the correlation is statistically significant, and the likelihood that this correlation is due to random chance is very low.\nPractical Significance: While statistically significant, the strength of the correlation (moderate) suggests that while there is a relationship, other factors also significantly influence the presence of flooding. It's important to consider these factors in any risk assessment or land use planning.\nFurther Analysis: Investigating how other variables interact with 'Drainage' and 'Flooding' could provide deeper insights. For instance, soil texture, slope, and capability class could be influencing how quickly water is absorbed or runs off, affecting flooding.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'randomForest' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'randomForest'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\ncolnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"County\"                     \"County_MAPSYM\"             \n [3] \"MAPSYM\"                     \"MUKEY\"                     \n [5] \"Default.Mineral.Soil.Group\" \"Multiple.MSG.Flag\"         \n [7] \"Flag...MSG.Values\"          \"Flag...Fields\"             \n [9] \"Capability.Class..FM5.CAP.\" \"Soil.Temp..Regime\"         \n[11] \"Soil.Modifier\"              \"Soil.Slope\"                \n[13] \"Soil.Name\"                  \"Drainage\"                  \n[15] \"Modifier\"                   \"Texture\"                   \n[17] \"Flooding\"                   \"Lime\"                      \n[19] \"Rotation\"                   \"Corn.Yield..ton.acre.\"     \n[21] \"Hay.Yield..ton.acre.\"       \"Change\"                    \n[23] \"TDN..ton.acre.\"             \"Index..TDN.\"               \n```\n\n\n:::\n\n```{.r .cell-code}\n# Handling NA values - assuming you're interested in predicting 'Flooding'\ndata_clean <- na.omit(data[, c(\"Flooding\", \"Drainage\", \"Texture\", \"Soil.Slope\", \"Capability.Class..FM5.CAP.\")])\ndata_clean$Texture <- as.numeric(as.factor(data_clean$Texture))\n\n\n\n# Convert all categorical variables to factor type\ndata_clean$Capability_Class <- as.factor(data_clean$\"Capability.Class..FM5.CAP.\")\n\n\n# Fit Random Forest model\nset.seed(123)  # for reproducibility\nrf_model <- randomForest(Flooding ~ ., data=data_clean, ntree=500, importance=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print model summary\nprint(rf_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Flooding ~ ., data = data_clean, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.08768223\n                    % Var explained: 91.48\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot importance of variables\nvarImpPlot(rf_model)\n```\n\n::: {.cell-output-display}\n![](influence_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nThese plots are key to understanding which predictors are most influential in modeling the outcome (in your case, flooding).\n\nInterpretation of the Variable Importance Plots:\n%IncMSE: This plot shows the increase in Mean Squared Error (MSE) of the model when each variable is randomly shuffled. A higher value indicates that the model relies more on that variable for prediction, meaning the variable is more important. According to your graph, 'Drainage' seems to be the most important predictor, followed by 'Capability_Class' and 'Texture'.\nIncNodePurity: This measure is based on the total decrease in node impurities from splitting on the variable, averaged over all trees. Node impurity is typically measured by the RSS (regression) or Gini impurity (classification). In this graph, 'Capability_Class' contributes most to node purity, followed by 'Drainage' and 'Texture'. This suggests that 'Capability_Class' is particularly effective at creating homogeneous nodes, likely due to its role in determining soil usability.\n\n#Investigating interaction effects between predictors like 'Drainage' and 'Texture' in the context of their impact on flooding can provide deeper insights into how these variables jointly influence the outcome. In R, you can include interaction terms directly in your model formula to study these effects. Here, we'll look at two approaches: using multiple regression to evaluate the statistical significance of the interaction, and using a Random Forest model to assess the predictive power when interactions are considered.\n\nR Code for Multiple Regression with Interaction Terms\nWe'll modify the linear regression model to include an interaction term between 'Drainage' and 'Texture'. This will allow us to see if the effect of one variable on flooding depends on the level of the other variable.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary library\nlibrary(stats)\n\ndata$Texture <- as.numeric(as.factor(data$Texture))  # Convert categorical to numeric\ndata$Drainage <- as.numeric(as.factor(data$Drainage))  # Convert categorical to numeric if needed\n\n# Fit Multiple Regression Model with Interaction Term\nmodel_interaction <- lm(Flooding ~ Drainage * Texture, data = data)\n\n# Summary of the model to see coefficients and significance\nsummary(model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Flooding ~ Drainage * Texture, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0282 -0.2530 -0.1270  0.3186  2.4724 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      -0.349160   0.133186  -2.622  0.00908 ** \nDrainage          0.229149   0.015232  15.044  < 2e-16 ***\nTexture           0.294757   0.020929  14.084  < 2e-16 ***\nDrainage:Texture -0.020150   0.002235  -9.018  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6614 on 405 degrees of freedom\nMultiple R-squared:  0.5791,\tAdjusted R-squared:  0.576 \nF-statistic: 185.8 on 3 and 405 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\nInterpretation of the Regression Output\nCoefficients:\nIntercept (-0.349160): This represents the baseline value of flooding when both 'Drainage' and 'Texture' are at their reference levels (typically zero in numerical coding).\nDrainage (0.229149): This coefficient indicates that for each unit increase in drainage (without considering the impact of texture), flooding increases by approximately 0.229 units, holding other factors constant.\nTexture (0.294757): Similarly, for each unit increase in texture, flooding increases by approximately 0.295 units, holding other factors constant.\nDrainage:Texture Interaction (-0.020150): The negative interaction term suggests that the combined effect of 'Drainage' and 'Texture' on flooding is less than the sum of their individual effects. In other words, higher levels of one may slightly mitigate the influence of the other on flooding.\nStatistical Significance:\nAll predictors, including the interaction term, are highly statistically significant (p < 0.001), indicating strong evidence against the null hypothesis of no effect.\nModel Fit:\nResidual Standard Error (RSE) (0.6614): This measures the typical size of the residuals, and in your context, it implies that the standard deviation of the residuals is around 0.661 units.\nMultiple R-squared (0.5791): Approximately 57.91% of the variability in flooding is explained by the model, which is a decent level of explanatory power for natural science data.\nAdjusted R-squared (0.576): Slightly adjusted for the number of predictors, still indicating a good fit.\nResiduals:\nThe spread of residuals suggests that while the model fits well for many observations (median close to zero), there are outliers and some predictions that deviate significantly from the actual values, as indicated by the min and max residuals.\n\n#R Code for Random Forest with Feature Engineering\nRandom Forest inherently considers interactions among features, but we can explicitly engineer an interaction feature to see how it influences model performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(dplyr)\n\n# Prepare the data\ndata <- mutate(data, Drainage_Texture_Interaction = Drainage * Texture)\n\n# Fit Random Forest Model including the engineered interaction feature\nset.seed(123)  # for reproducibility\nrf_model_interaction <- randomForest(Flooding ~ Drainage + Texture + Drainage_Texture_Interaction, data = data, ntree = 500)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n```\n\n\n:::\n\n```{.r .cell-code}\n# Evaluate the model (assuming Flooding is continuous; adjust as necessary)\nprint(rf_model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Flooding ~ Drainage + Texture + Drainage_Texture_Interaction,      data = data, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1969851\n                    % Var explained: 80.86\n```\n\n\n:::\n\n```{.r .cell-code}\n# Optionally, view the importance of the new interaction feature\nimportance(rf_model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             IncNodePurity\nDrainage                          115.0117\nTexture                           121.6274\nDrainage_Texture_Interaction      131.4226\n```\n\n\n:::\n:::\n\n\nInterpretation of the Random Forest Model Results:\nModel Performance:\nMean of squared residuals (0.1969851): This value is considerably lower than the residual standard error from the linear regression model (0.6614), suggesting that the Random Forest model has better predictive accuracy.\n% Var explained (80.86%): A high percentage of variance explained indicates that the Random Forest model is effectively capturing the relationships and variability in the data. It explains more than 80% of the variance in flooding, which is significantly higher than the Multiple R-squared from the linear regression model (57.91%).\nFeature Importance:\nDrainage: Importance score of 115.0117, suggesting it's a significant predictor of flooding.\nTexture: Importance score of 121.6274, slightly more influential than 'Drainage'.\nDrainage_Texture_Interaction: With the highest importance score of 131.4226, this engineered feature seems to be the most significant predictor in the model. This underscores the value of including interaction terms explicitly, even in a model like Random Forest that inherently accounts for interactions among features.\nComparison and Conclusion:\nExplanatory Power: The linear regression model provides clear coefficients that describe the relationship between each predictor and the outcome, including how the interaction term modifies these relationships. This is particularly useful for hypothesis testing and understanding the specific effects of changes in predictors.\nPredictive Accuracy: The Random Forest model outperforms the linear regression in terms of predictive accuracy, explaining a higher percentage of the variance in flooding and producing a lower mean squared residual.\nFeature Importance: Random Forest offers an advantage in evaluating the importance of features, including interactions, without needing a specific hypothesis about their effects.\n\n\n# Cross-Validation and Residual Analysis\n1. Cross-Validation for Linear Regression and Random Forest\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'caret' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'ggplot2'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:randomForest':\n\n    margin\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\n\ndata$Texture <- as.numeric(as.factor(data$Texture))\ndata$Drainage <- as.numeric(as.factor(data$Drainage))\n\n# Define training control\ntrain_control <- trainControl(method = \"cv\", number = 10)  # 10-fold cross-validation\n\n# Fit Linear Regression Model using cross-validation\nlm_model_cv <- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"lm\", trControl = train_control)\n\n# Fit Random Forest Model using cross-validation\nrf_model_cv <- train(Flooding ~ Drainage + Texture + Drainage:Texture, data = data, method = \"rf\", trControl = train_control, ntree = 500)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\nWarning in randomForest.default(x, y, mtry = param$mtry, ...): The response has\nfive or fewer unique values.  Are you sure you want to do regression?\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary of cross-validation results\nprint(lm_model_cv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 367, 368, 368, 368, 369, 367, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.6608545  0.5871187  0.4143339\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(rf_model_cv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n409 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 368, 369, 368, 369, 368, 367, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.4282331  0.8134469  0.1653817\n  3     0.4379896  0.8039825  0.1682584\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n```\n\n\n:::\n:::\n\nThe cross-validation results for both the Linear Regression and Random Forest models provide a clear comparison of their performance:\n\nLinear Regression Model:\nRMSE (Root Mean Squared Error): 0.6608545\nR-squared: 0.5871187\nMAE (Mean Absolute Error): 0.4143339\nThese metrics indicate that the linear regression model explains about 58.71% of the variance in the data. The RMSE and MAE values provide a measure of the average error in the predictions.\n\nRandom Forest Model:\nRMSE: 0.4282331\nR-squared: 0.8134469\nMAE: 0.1653817\nOptimal mtry: 2\nThe Random Forest model significantly outperforms the Linear Regression in all the metrics. It explains approximately 81.34% of the variance in the data, and both its RMSE and MAE are lower, indicating more accurate predictions.\n\n# Diagnostic Plots for Linear Regression\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nlm_model <- lm(Flooding ~ Drainage + Texture + Drainage:Texture, data = data)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(lm_model)\n```\n\n::: {.cell-output-display}\n![](influence_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThe diagnostic plots for your linear regression model provide valuable insights into how well the model meets the assumptions necessary for optimal performance. Here's an interpretation of each plot:\n\n1. Residuals vs Fitted\nThis plot helps check for non-linearity and heteroscedasticity (unequal variance of residuals).\n\nObservations: The residuals do not appear to display any clear pattern, which is good for linearity. However, there is a slight \"fanning\" effect where the spread of residuals increases with fitted values, suggesting potential heteroscedasticity.\nAction: Consider transformations of the dependent variable or use heteroscedasticity-consistent standard errors if this model will be used for inferential purposes.\n2. Normal Q-Q\nThis plot shows if the residuals are normally distributedâ€”a key assumption of linear regression.\n\nObservations: Most points lie on the line, but there are deviations at the tails (both lower and upper ends), indicating slight departures from normality.\nAction: This is generally not severe unless very precise estimates are required. For more robustness, consider using non-parametric bootstrapping techniques to estimate standard errors.\n3. Scale-Location (or Spread-Location)\nThis plot checks if residuals are spread equally along the ranges of predictors (homoscedasticity).\n\nObservations: The red line (a loess fit) shows a trend, which suggests that residuals have non-constant variance across the range of fitted values.\nAction: This supports the earlier suggestion of possible heteroscedasticity. Transformations or robust regression methods might be needed.\n4. Residuals vs Leverage\nThis plot helps to identify influential cases that might have an unduly large effect on the model estimate.\n\nObservations: Most data points have low leverage, but there are a few points well outside the Cook's distance lines (notably the points labeled 7280 and 4430).\nAction: Investigate these points further to determine if they are outliers or influential points due to data entry errors or other reasons. Consider removing or adjusting these points if they are deemed to be errors.\nConclusion\nThe diagnostic plots indicate that while the model does not suffer from severe issues, there are indications of potential heteroscedasticity and some influence from outliers or high-leverage points.\n\n",
    "supporting": [
      "influence_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}